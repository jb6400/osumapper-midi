{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2019/4/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "#### First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://i.imgur.com/Ko2wogO.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "Note that this GAN is irrelevant to the music, not the time interval, only the coordinates of notes themselves.\n",
    "\n",
    "Probably could get some slider coordinates inbetween? this way it may learn something about slider shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Tensorflow 2.0 enables eager automatically\n",
    "try:\n",
    "    tf.enable_eager_execution();\n",
    "    tfe = tf.contrib.eager;\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters related to GAN training.\n",
    "\n",
    "May or may not affect the result; haven't experimented a lot about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_PARAMS = {\n",
    "    \"divisor\" : 4,\n",
    "    \"good_epoch\" : 6,\n",
    "    \"max_epoch\" : 25,\n",
    "    \"note_group_size\" : 10,\n",
    "    \"g_epochs\" : 7,\n",
    "    \"c_epochs\" : 3,\n",
    "    \"g_batch\" : 50,\n",
    "    \"g_input_size\" : 50,\n",
    "    \"c_true_batch\" : 50,\n",
    "    \"c_false_batch\" : 10,\n",
    "    \"slider_max_ticks\" : 8,\n",
    "    \"next_from_slider_end\" : False\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "#### Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, unfiltered_sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "first_step_sv =          unfiltered_sv[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "slider_max_ticks = GAN_PARAMS[\"slider_max_ticks\"];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + slider_max_ticks + 1: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "sv =          first_step_sv[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# get divisor from GAN_PARAMS\n",
    "divisor = GAN_PARAMS[\"divisor\"];\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // divisor;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 5, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "slider_cos_each = slider_cos[slider_types];\n",
    "slider_sin_each = slider_sin[slider_types];\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For distances starting from slider ends\n",
    "tick_lengths_pre = (timestamps[1:] - timestamps[:-1]) / (ticks[1:] - ticks[:-1]);\n",
    "tick_lengths = np.concatenate([tick_lengths_pre, [tick_lengths_pre[-1]]]);\n",
    "timestamps_note_end = timestamps + slider_ticks * tick_lengths;\n",
    "\n",
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "\n",
    "if GAN_PARAMS[\"next_from_slider_end\"]:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps_note_end;\n",
    "    timestamps_before = np.concatenate([[6662], timestamps_after[:-1]]); # why 6662????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "else:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps;\n",
    "    timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting functions (only for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "from plthelper import MyLine, plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"mapdata/\";\n",
    "\n",
    "chunk_size = GAN_PARAMS[\"note_group_size\"];\n",
    "step_size = 5;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.001) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "from tfhelper import *\n",
    "\n",
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    wall_var_l = tf.where(tf.less(vg, 0.2), tf.square(0.3 - vg), 0 * vg);\n",
    "    wall_var_r = tf.where(tf.greater(vg, 0.8), tf.square(vg - 0.7), 0 * vg);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    wall_var_l = tf.cast(tf.less(vg, 0), tf.float32);\n",
    "    wall_var_r = tf.cast(tf.greater(vg, 1), tf.float32);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "    var_tensor = tf.cast(var_tensor, tf.float32);\n",
    "    var_shape = var_tensor.shape;\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    \n",
    "#     note_distances_now = length_multiplier * np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0);\n",
    "#     note_angles_now = np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0);\n",
    "    \n",
    "    relevant_tensors = extvar[\"relevant_tensors\"];\n",
    "    relevant_is_slider =      relevant_tensors[\"is_slider\"];\n",
    "    relevant_slider_lengths = relevant_tensors[\"slider_lengths\"];\n",
    "    relevant_slider_types =   relevant_tensors[\"slider_types\"];\n",
    "    relevant_slider_cos =     relevant_tensors[\"slider_cos_each\"];\n",
    "    relevant_slider_sin =     relevant_tensors[\"slider_sin_each\"];\n",
    "    relevant_note_distances = relevant_tensors[\"note_distances\"];\n",
    "    relevant_note_angles =    relevant_tensors[\"note_angles\"];\n",
    "    \n",
    "    note_distances_now = length_multiplier * tf.expand_dims(relevant_note_distances, axis=0);\n",
    "    note_angles_now = tf.expand_dims(relevant_note_angles, axis=0);\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "    \n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "    \n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.cast(tf.greater(l, y_max / 2), tf.float32);\n",
    "    not_rerand = tf.cast(tf.less_equal(l, y_max / 2), tf.float32);\n",
    "    \n",
    "    next_from_slider_end = extvar[\"next_from_slider_end\"];\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _pre_px = extvar[\"start_pos\"][0];\n",
    "        _pre_py = extvar[\"start_pos\"][1];\n",
    "        _px = tf.cast(_pre_px, tf.float32);\n",
    "        _py = tf.cast(_pre_py, tf.float32);\n",
    "    else:\n",
    "        _px = tf.cast(256, tf.float32);\n",
    "        _py = tf.cast(192, tf.float32);\n",
    "    \n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = tf.cast(256, tf.float32);\n",
    "    _y = tf.cast(192, tf.float32);\n",
    "    \n",
    "    outputs = tf.TensorArray(tf.float32, half_tensor)\n",
    "\n",
    "    for k in range(half_tensor):\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l =    tf.cast(tf.less(_px, wall_l[:, k]), tf.float32);\n",
    "        wall_value_r =    tf.cast(tf.greater(_px, wall_r[:, k]), tf.float32);\n",
    "        wall_value_xmid = tf.cast(tf.greater(_px, wall_l[:, k]), tf.float32) * tf.cast(tf.less(_px, wall_r[:, k]), tf.float32);\n",
    "        wall_value_t =    tf.cast(tf.less(_py, wall_t[:, k]), tf.float32);\n",
    "        wall_value_b =    tf.cast(tf.greater(_py, wall_b[:, k]), tf.float32);\n",
    "        wall_value_ymid = tf.cast(tf.greater(_py, wall_t[:, k]), tf.float32) * tf.cast(tf.less(_py, wall_b[:, k]), tf.float32);\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        \n",
    "        # slider part\n",
    "        sln = relevant_slider_lengths[k];\n",
    "        slider_type = relevant_slider_types[k];\n",
    "        scos = relevant_slider_cos[k];\n",
    "        ssin = relevant_slider_sin[k];\n",
    "        _a = cos_list[:, k + half_tensor];\n",
    "        _b = sin_list[:, k + half_tensor];\n",
    "        # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "        # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "        _oa = _a * scos - _b * ssin;\n",
    "        _ob = _a * ssin + _b * scos;\n",
    "        cp_slider = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "        _px_slider = tf.cond(next_from_slider_end, lambda: _x + _a * sln, lambda: _x);\n",
    "        _py_slider = tf.cond(next_from_slider_end, lambda: _y + _b * sln, lambda: _y);\n",
    "        \n",
    "        # circle part\n",
    "        _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "        _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "        cp_circle = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "        _px_circle = _x;\n",
    "        _py_circle = _y;\n",
    "        \n",
    "        outputs = outputs.write(k, tf.where(relevant_is_slider[k], cp_slider, cp_circle))\n",
    "        _px = tf.where(tf.cast(relevant_is_slider[k], tf.bool), _px_slider, _px_circle)\n",
    "        _py = tf.where(tf.cast(relevant_is_slider[k], tf.bool), _py_slider, _py_circle)\n",
    "\n",
    "    return tf.transpose(outputs.stack(), [1, 0, 2]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "\n",
    "# Loss functions and mapping layer, to adapt to TF 2.0\n",
    "class GenerativeCustomLoss(LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def loss_function_for_generative_model(y_true, y_pred):\n",
    "            classification = y_pred;\n",
    "            loss1 = 1 - tf.reduce_mean(classification, axis=1);\n",
    "            return loss1;\n",
    "        \n",
    "        super(GenerativeCustomLoss, self).__init__(loss_function_for_generative_model, name=name, reduction=reduction)\n",
    "\n",
    "class BoxCustomLoss(LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def box_loss(y_true, y_pred):\n",
    "            map_part = y_pred;\n",
    "            return inblock_loss(map_part[:, :, 0:2]) + inblock_loss(map_part[:, :, 4:6])\n",
    "        \n",
    "        super(BoxCustomLoss, self).__init__(box_loss, name=name, reduction=reduction)\n",
    "\n",
    "class AlwaysZeroCustomLoss(LossFunctionWrapper): # why does TF not include this! this is very important in certain situations\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def alw_zero(y_true, y_pred):\n",
    "            return tf.convert_to_tensor(0, dtype=tf.float32);\n",
    "        \n",
    "        super(AlwaysZeroCustomLoss, self).__init__(alw_zero, name=name, reduction=reduction)\n",
    "        \n",
    "        \n",
    "class KerasCustomMappingLayer(keras.layers.Layer):\n",
    "    def __init__(self, extvar, output_shape=(special_train_data.shape[1], special_train_data.shape[2]), *args, **kwargs):\n",
    "        self.extvar = extvar\n",
    "        self._output_shape = output_shape\n",
    "        self.extvar_begin = tf.Variable(tf.convert_to_tensor(extvar[\"begin\"], dtype=tf.int32), trainable=False)\n",
    "        self.extvar_lmul =  tf.Variable(tf.convert_to_tensor([extvar[\"length_multiplier\"]], dtype=tf.float32), trainable=False)\n",
    "        self.extvar_nfse =  tf.Variable(tf.convert_to_tensor(extvar[\"next_from_slider_end\"], dtype=tf.bool), trainable=False)\n",
    "        self.note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "        \n",
    "        self.extvar_spos =  tf.Variable(tf.cast(tf.zeros((2, )), tf.float32), trainable=False)\n",
    "        self.extvar_rel =   tf.Variable(tf.cast(tf.zeros((7, self.note_group_size)), tf.float32), trainable=False)\n",
    "        \n",
    "        super(KerasCustomMappingLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape): # since this is a static layer, no building is required\n",
    "        pass\n",
    "    \n",
    "    def set_extvar(self, extvar):\n",
    "        self.extvar = extvar;\n",
    "        \n",
    "        # Populate extvar with the rel variable (this will modify the input extvar)\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "        self.extvar[\"relevant_tensors\"] = {\n",
    "            \"is_slider\"       : tf.convert_to_tensor(is_slider      [begin_offset : begin_offset + self.note_group_size], dtype=tf.bool),\n",
    "            \"slider_lengths\"  : tf.convert_to_tensor(slider_lengths [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_types\"    : tf.convert_to_tensor(slider_types   [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_cos_each\" : tf.convert_to_tensor(slider_cos_each[begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_sin_each\" : tf.convert_to_tensor(slider_sin_each[begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"note_distances\" :  tf.convert_to_tensor(note_distances [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"note_angles\" :     tf.convert_to_tensor(note_angles    [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32)\n",
    "        };\n",
    "        \n",
    "        # Continue\n",
    "        self.extvar_begin.assign(extvar[\"begin\"])\n",
    "        self.extvar_spos.assign(extvar[\"start_pos\"])\n",
    "        self.extvar_lmul.assign([extvar[\"length_multiplier\"]])\n",
    "        self.extvar_nfse.assign(extvar[\"next_from_slider_end\"])\n",
    "        self.extvar_rel.assign(tf.convert_to_tensor([\n",
    "            is_slider      [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_lengths [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_types   [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_cos_each[begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_sin_each[begin_offset : begin_offset + self.note_group_size],\n",
    "            note_distances [begin_offset : begin_offset + self.note_group_size],\n",
    "            note_angles    [begin_offset : begin_offset + self.note_group_size]\n",
    "        ], dtype=tf.float32))\n",
    "\n",
    "    # Call method will sometimes get used in graph mode,\n",
    "    # training will get turned into a tensor\n",
    "#     @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        mapvars = inputs;\n",
    "        start_pos = self.extvar_spos\n",
    "        rel = self.extvar_rel\n",
    "        extvar = {\n",
    "            \"begin\" : self.extvar_begin,\n",
    "            # \"start_pos\" : self.extvar_start_pos,\n",
    "            \"start_pos\" : tf.cast(start_pos, tf.float32),\n",
    "            \"length_multiplier\" : self.extvar_lmul,\n",
    "            \"next_from_slider_end\" : self.extvar_nfse,\n",
    "            # \"relevant_tensors\" : self.extvar_rel\n",
    "            \"relevant_tensors\" : {\n",
    "                \"is_slider\"       : tf.cast(rel[0], tf.bool),\n",
    "                \"slider_lengths\"  : tf.cast(rel[1], tf.float32),\n",
    "                \"slider_types\"    : tf.cast(rel[2], tf.float32),\n",
    "                \"slider_cos_each\" : tf.cast(rel[3], tf.float32),\n",
    "                \"slider_sin_each\" : tf.cast(rel[4], tf.float32),\n",
    "                \"note_distances\"  : tf.cast(rel[5], tf.float32),\n",
    "                \"note_angles\"     : tf.cast(rel[6], tf.float32)\n",
    "            }\n",
    "        }\n",
    "        result = construct_map_with_sliders(mapvars, extvar=extvar);\n",
    "        return result;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "This will take some time. It splits map into groups of 10 (10 by default), and trains GAN to play with the flow of maps in the dataset. It will train for floor(note_count / 10) groups, for example, if you have 820 then it needs to train 82 groups.\n",
    "\n",
    "Some notes are converted to sliders, so it has less than the predicted count in #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XdfkR223D9dW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of groups: 102\n",
      "Group 0, Epoch 1: G loss: 0.23401742960725516 vs. C loss: 0.13211140698856777\n",
      "Group 0, Epoch 2: G loss: 0.4271032278026854 vs. C loss: 0.1167684288488494\n",
      "Group 0, Epoch 3: G loss: 0.597323214156287 vs. C loss: 0.07344902399927378\n",
      "Group 0, Epoch 4: G loss: 0.442962972181184 vs. C loss: 0.21641269326210022\n",
      "Group 0, Epoch 5: G loss: 0.039398251101374626 vs. C loss: 0.2079091386662589\n",
      "Group 0, Epoch 6: G loss: 0.012072156263249261 vs. C loss: 0.20794698554608557\n",
      "Group 1, Epoch 1: G loss: 0.2528233630316598 vs. C loss: 0.19435472124152717\n",
      "Group 1, Epoch 2: G loss: 0.2840580723115376 vs. C loss: 0.1654603232940038\n",
      "Group 1, Epoch 3: G loss: 0.4662288418837956 vs. C loss: 0.068089683747126\n",
      "Group 1, Epoch 4: G loss: 0.7532181007521495 vs. C loss: 0.04002372444503837\n",
      "Group 1, Epoch 5: G loss: 0.9415880833353316 vs. C loss: 0.0082887457166281\n",
      "Group 1, Epoch 6: G loss: 0.9815475719315664 vs. C loss: 0.0009486053830995742\n",
      "Group 1, Epoch 7: G loss: 0.9882785660879951 vs. C loss: 0.0018662237141850507\n",
      "Group 1, Epoch 8: G loss: 0.9676016058240619 vs. C loss: 0.000991909421120201\n",
      "Group 1, Epoch 9: G loss: 0.7861530344401089 vs. C loss: 0.13496297339184418\n",
      "Group 1, Epoch 10: G loss: 1.0246722493852887 vs. C loss: 0.0647553503513336\n",
      "Group 1, Epoch 11: G loss: 0.5650931569082396 vs. C loss: 0.2089920499258571\n",
      "Group 1, Epoch 12: G loss: 0.033574292436242104 vs. C loss: 0.2081969678401947\n",
      "Group 2, Epoch 1: G loss: 0.2728046762091773 vs. C loss: 0.1860237494111061\n",
      "Group 2, Epoch 2: G loss: 0.3421217037098749 vs. C loss: 0.13687227749162248\n",
      "Group 2, Epoch 3: G loss: 0.4824890639100756 vs. C loss: 0.09297175208727519\n",
      "Group 2, Epoch 4: G loss: 0.5677191512925284 vs. C loss: 0.073019925918844\n",
      "Group 2, Epoch 5: G loss: 0.988507640361786 vs. C loss: 0.012880086588362852\n",
      "Group 2, Epoch 6: G loss: 0.6260452798434665 vs. C loss: 0.05512575505094396\n",
      "Group 3, Epoch 1: G loss: 0.28783637157508307 vs. C loss: 0.18730070855882433\n",
      "Group 3, Epoch 2: G loss: 0.1777583360671997 vs. C loss: 0.19050371398528418\n",
      "Group 3, Epoch 3: G loss: 0.1396474044237818 vs. C loss: 0.14598540630605486\n",
      "Group 3, Epoch 4: G loss: 0.47962573511259904 vs. C loss: 0.12084766146209507\n",
      "Group 3, Epoch 5: G loss: 0.37431307275380404 vs. C loss: 0.23305208318763312\n",
      "Group 3, Epoch 6: G loss: 0.02146129405924252 vs. C loss: 0.20285652743445504\n",
      "Group 4, Epoch 1: G loss: 0.25149026172501704 vs. C loss: 0.1812151943643888\n",
      "Group 4, Epoch 2: G loss: 0.27351412177085876 vs. C loss: 0.1864238017135196\n",
      "Group 4, Epoch 3: G loss: 0.14237291451011383 vs. C loss: 0.1553237893515163\n",
      "Group 4, Epoch 4: G loss: 0.4536112044538771 vs. C loss: 0.10914348148637347\n",
      "Group 4, Epoch 5: G loss: 0.6505128664629799 vs. C loss: 0.06567422321273221\n",
      "Group 4, Epoch 6: G loss: 0.4663519088711057 vs. C loss: 0.18231056382258734\n",
      "Group 5, Epoch 1: G loss: 0.2733560689858028 vs. C loss: 0.2010222872098287\n",
      "Group 5, Epoch 2: G loss: 0.2630400568246841 vs. C loss: 0.16486346059375337\n",
      "Group 5, Epoch 3: G loss: 0.3901167852537973 vs. C loss: 0.09027944194773833\n",
      "Group 5, Epoch 4: G loss: 0.5615871050528117 vs. C loss: 0.10444825184014107\n",
      "Group 5, Epoch 5: G loss: 0.6948378903525215 vs. C loss: 0.05238006429539787\n",
      "Group 5, Epoch 6: G loss: 0.6560428415025983 vs. C loss: 0.06032817487397956\n",
      "Group 6, Epoch 1: G loss: 0.31392398221152173 vs. C loss: 0.1807965222332213\n",
      "Group 6, Epoch 2: G loss: 0.2745779595204762 vs. C loss: 0.1624504890706804\n",
      "Group 6, Epoch 3: G loss: 0.4847449140889304 vs. C loss: 0.13580293373929128\n",
      "Group 6, Epoch 4: G loss: 0.565199122258595 vs. C loss: 0.08169132481432623\n",
      "Group 6, Epoch 5: G loss: 0.8850986225264412 vs. C loss: 0.027972515672445297\n",
      "Group 6, Epoch 6: G loss: 0.4046778106263705 vs. C loss: 0.20791629453500113\n",
      "Group 7, Epoch 1: G loss: 0.2754168633903776 vs. C loss: 0.1654370211892658\n",
      "Group 7, Epoch 2: G loss: 0.20578220805951528 vs. C loss: 0.18328597148259482\n",
      "Group 7, Epoch 3: G loss: 0.14032704276697977 vs. C loss: 0.19226018091042837\n",
      "Group 7, Epoch 4: G loss: 0.34848675302096777 vs. C loss: 0.09268484388788541\n",
      "Group 7, Epoch 5: G loss: 0.5099931512560164 vs. C loss: 0.19457577831215325\n",
      "Group 7, Epoch 6: G loss: 0.09902474124516758 vs. C loss: 0.2061946541070938\n",
      "Group 8, Epoch 1: G loss: 0.28110454763684956 vs. C loss: 0.16909705764717528\n",
      "Group 8, Epoch 2: G loss: 0.2582270639283316 vs. C loss: 0.18762262579467562\n",
      "Group 8, Epoch 3: G loss: 0.09091743060520717 vs. C loss: 0.19400978336731592\n",
      "Group 8, Epoch 4: G loss: 0.10519811915499823 vs. C loss: 0.15383079979154798\n",
      "Group 8, Epoch 5: G loss: 0.43353852970259527 vs. C loss: 0.08873124751779767\n",
      "Group 8, Epoch 6: G loss: 0.6245010495185852 vs. C loss: 0.029228284541103575\n",
      "Group 9, Epoch 1: G loss: 0.28602876450334275 vs. C loss: 0.17191648731629053\n",
      "Group 9, Epoch 2: G loss: 0.23583392798900604 vs. C loss: 0.17638668914635977\n",
      "Group 9, Epoch 3: G loss: 0.25690899789333344 vs. C loss: 0.0840294717086686\n",
      "Group 9, Epoch 4: G loss: 0.8790474329675947 vs. C loss: 0.029337931838300493\n",
      "Group 9, Epoch 5: G loss: 0.3938233425574644 vs. C loss: 0.2038581089840995\n",
      "Group 9, Epoch 6: G loss: 0.02706199479954583 vs. C loss: 0.19758481201198366\n",
      "Group 10, Epoch 1: G loss: 0.2745897561311722 vs. C loss: 0.18844577504528892\n",
      "Group 10, Epoch 2: G loss: 0.32252219702516277 vs. C loss: 0.16339900013473296\n",
      "Group 10, Epoch 3: G loss: 0.3669538038117545 vs. C loss: 0.07113965683513217\n",
      "Group 10, Epoch 4: G loss: 0.8398534808840071 vs. C loss: 0.019047938235518005\n",
      "Group 10, Epoch 5: G loss: 0.9135071464947293 vs. C loss: 0.0055785951101117665\n",
      "Group 10, Epoch 6: G loss: 0.9132774846894399 vs. C loss: 0.00403291046515935\n",
      "Group 10, Epoch 7: G loss: 1.003866536276681 vs. C loss: 0.0003676401287925223\n",
      "Group 10, Epoch 8: G loss: 1.0151925802230835 vs. C loss: 0.004429651846294291\n",
      "Group 10, Epoch 9: G loss: 0.612998389346259 vs. C loss: 0.06925790970368932\n",
      "Group 10, Epoch 10: G loss: 1.0315680503845213 vs. C loss: 0.0702120736370691\n",
      "Group 10, Epoch 11: G loss: 1.0301799365452358 vs. C loss: 0.04008130069511632\n",
      "Group 10, Epoch 12: G loss: 1.0253325155803137 vs. C loss: 0.0033010658614026998\n",
      "Group 10, Epoch 13: G loss: 0.9305405003683908 vs. C loss: 0.0033417906249976824\n",
      "Group 10, Epoch 14: G loss: 0.9720852749688284 vs. C loss: 0.0007795335162275782\n",
      "Group 10, Epoch 15: G loss: 1.0127516542162214 vs. C loss: 0.004393948998767883\n",
      "Group 10, Epoch 16: G loss: 0.9306453772953577 vs. C loss: 0.007113336145670879\n",
      "Group 10, Epoch 17: G loss: 0.3891145793454988 vs. C loss: 0.20816125927699938\n",
      "Group 10, Epoch 18: G loss: 0.04267171631966319 vs. C loss: 0.20843281514114806\n",
      "Group 10, Epoch 19: G loss: 0.03651388159820011 vs. C loss: 0.20832577596108118\n",
      "Group 11, Epoch 1: G loss: 0.28372199067047665 vs. C loss: 0.18951447639200425\n",
      "Group 11, Epoch 2: G loss: 0.27625047266483305 vs. C loss: 0.16523332397143045\n",
      "Group 11, Epoch 3: G loss: 0.22456105734620777 vs. C loss: 0.16305714017815057\n",
      "Group 11, Epoch 4: G loss: 0.2922349638172559 vs. C loss: 0.1547680956621965\n",
      "Group 11, Epoch 5: G loss: 0.3511246877057212 vs. C loss: 0.09350447729229927\n",
      "Group 11, Epoch 6: G loss: 0.744387480190822 vs. C loss: 0.016987454999859132\n",
      "Group 12, Epoch 1: G loss: 0.2870423934289387 vs. C loss: 0.20245342784457734\n",
      "Group 12, Epoch 2: G loss: 0.21425354863916124 vs. C loss: 0.15654012809197107\n",
      "Group 12, Epoch 3: G loss: 0.28234536009175437 vs. C loss: 0.15022795730166966\n",
      "Group 12, Epoch 4: G loss: 0.664381902558463 vs. C loss: 0.03288630199515157\n",
      "Group 12, Epoch 5: G loss: 0.9019817556653704 vs. C loss: 0.01170111033651564\n",
      "Group 12, Epoch 6: G loss: 0.8020076734679086 vs. C loss: 0.004708023521945708\n",
      "Group 13, Epoch 1: G loss: 0.29598934948444366 vs. C loss: 0.18350373788012397\n",
      "Group 13, Epoch 2: G loss: 0.30963848531246185 vs. C loss: 0.13253723084926602\n",
      "Group 13, Epoch 3: G loss: 0.4488351877246584 vs. C loss: 0.15393846192293698\n",
      "Group 13, Epoch 4: G loss: 0.37302254693848746 vs. C loss: 0.11606980425616105\n",
      "Group 13, Epoch 5: G loss: 0.8654211367879595 vs. C loss: 0.07393600005242561\n",
      "Group 13, Epoch 6: G loss: 0.5280777392642838 vs. C loss: 0.206953548722797\n",
      "Group 14, Epoch 1: G loss: 0.27734322462763106 vs. C loss: 0.1906071321831809\n",
      "Group 14, Epoch 2: G loss: 0.2588131355387824 vs. C loss: 0.15990236732694837\n",
      "Group 14, Epoch 3: G loss: 0.2599682497126716 vs. C loss: 0.17502515183554757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 14, Epoch 4: G loss: 0.21170581621783116 vs. C loss: 0.21529117888874474\n",
      "Group 14, Epoch 5: G loss: 0.11475088724068234 vs. C loss: 0.15235797233051726\n",
      "Group 14, Epoch 6: G loss: 0.37857506317751743 vs. C loss: 0.10705778147611354\n",
      "Group 15, Epoch 1: G loss: 0.2680170727627618 vs. C loss: 0.18081949816809761\n",
      "Group 15, Epoch 2: G loss: 0.19047763092177256 vs. C loss: 0.19399729867776236\n",
      "Group 15, Epoch 3: G loss: 0.06692907341888973 vs. C loss: 0.17189821269777086\n",
      "Group 15, Epoch 4: G loss: 0.3424652712685721 vs. C loss: 0.10793292687998879\n",
      "Group 15, Epoch 5: G loss: 0.5190486252307892 vs. C loss: 0.05081299402647548\n",
      "Group 15, Epoch 6: G loss: 0.45059298542993415 vs. C loss: 0.21283833351400164\n",
      "Group 16, Epoch 1: G loss: 0.29960889901433674 vs. C loss: 0.1885956608586841\n",
      "Group 16, Epoch 2: G loss: 0.17765999627964837 vs. C loss: 0.18829707884126237\n",
      "Group 16, Epoch 3: G loss: 0.1493008960570608 vs. C loss: 0.1511122683684031\n",
      "Group 16, Epoch 4: G loss: 0.4260825991630554 vs. C loss: 0.08066233661439684\n",
      "Group 16, Epoch 5: G loss: 0.7854967883655003 vs. C loss: 0.028882483123905133\n",
      "Group 16, Epoch 6: G loss: 0.7869049974850247 vs. C loss: 0.0743934796967854\n",
      "Group 17, Epoch 1: G loss: 0.3141409320490701 vs. C loss: 0.15587549739413792\n",
      "Group 17, Epoch 2: G loss: 0.27018039034945623 vs. C loss: 0.18197738627592722\n",
      "Group 17, Epoch 3: G loss: 0.17816688333238875 vs. C loss: 0.13668406423595217\n",
      "Group 17, Epoch 4: G loss: 0.7012915900775365 vs. C loss: 0.041974359916316144\n",
      "Group 17, Epoch 5: G loss: 0.6837541673864637 vs. C loss: 0.051305937063362866\n",
      "Group 17, Epoch 6: G loss: 0.7144484843526567 vs. C loss: 0.06755042500380014\n",
      "Group 18, Epoch 1: G loss: 0.287379424061094 vs. C loss: 0.18459204501575896\n",
      "Group 18, Epoch 2: G loss: 0.3168632933071681 vs. C loss: 0.10508409846160148\n",
      "Group 18, Epoch 3: G loss: 0.4615869407142911 vs. C loss: 0.21100477874279022\n",
      "Group 18, Epoch 4: G loss: 0.030421511083841318 vs. C loss: 0.21576622790760466\n",
      "Group 18, Epoch 5: G loss: 0.012893874996474814 vs. C loss: 0.20408770690361658\n",
      "Group 18, Epoch 6: G loss: 0.018001409087862286 vs. C loss: 0.19846624632676443\n",
      "Group 19, Epoch 1: G loss: 0.28548422030040194 vs. C loss: 0.17278532104359734\n",
      "Group 19, Epoch 2: G loss: 0.21608411329133173 vs. C loss: 0.19394432836108735\n",
      "Group 19, Epoch 3: G loss: 0.06013399500932012 vs. C loss: 0.18513998554812539\n",
      "Group 19, Epoch 4: G loss: 0.17062411521162305 vs. C loss: 0.13575822280512914\n",
      "Group 19, Epoch 5: G loss: 0.5526705537523543 vs. C loss: 0.09591550545559989\n",
      "Group 19, Epoch 6: G loss: 0.7037233829498292 vs. C loss: 0.02526492780695359\n",
      "Group 20, Epoch 1: G loss: 0.2705827508653913 vs. C loss: 0.16383851733472612\n",
      "Group 20, Epoch 2: G loss: 0.16965131780930928 vs. C loss: 0.20492406851715514\n",
      "Group 20, Epoch 3: G loss: 0.04994505665131978 vs. C loss: 0.1914076970683204\n",
      "Group 20, Epoch 4: G loss: 0.08365543346319881 vs. C loss: 0.15035185383425817\n",
      "Group 20, Epoch 5: G loss: 0.4371166491082735 vs. C loss: 0.1990887307458454\n",
      "Group 20, Epoch 6: G loss: 0.3576774011765208 vs. C loss: 0.14892060889138115\n",
      "Group 21, Epoch 1: G loss: 0.30148576029709406 vs. C loss: 0.2030727333492703\n",
      "Group 21, Epoch 2: G loss: 0.23467356094292233 vs. C loss: 0.16253337429629433\n",
      "Group 21, Epoch 3: G loss: 0.22324600794485636 vs. C loss: 0.21308556861347625\n",
      "Group 21, Epoch 4: G loss: 0.07429751519645965 vs. C loss: 0.18615862354636192\n",
      "Group 21, Epoch 5: G loss: 0.1908341463123049 vs. C loss: 0.10944508595599069\n",
      "Group 21, Epoch 6: G loss: 0.7203528097697666 vs. C loss: 0.04308387326697508\n",
      "Group 21, Epoch 7: G loss: 0.6815391208444322 vs. C loss: 0.04317953323738443\n",
      "Group 22, Epoch 1: G loss: 0.2746229278189795 vs. C loss: 0.2108651614851422\n",
      "Group 22, Epoch 2: G loss: 0.2445377835205623 vs. C loss: 0.15926388402779898\n",
      "Group 22, Epoch 3: G loss: 0.5340864615780966 vs. C loss: 0.07696839918692906\n",
      "Group 22, Epoch 4: G loss: 0.45775997170380184 vs. C loss: 0.2072193018264241\n",
      "Group 22, Epoch 5: G loss: 0.030722453551633017 vs. C loss: 0.20219060364696714\n",
      "Group 22, Epoch 6: G loss: 0.0351500643151147 vs. C loss: 0.18690299656656054\n",
      "Group 23, Epoch 1: G loss: 0.252389104451452 vs. C loss: 0.21623030636045668\n",
      "Group 23, Epoch 2: G loss: 0.31695376123700825 vs. C loss: 0.13976605402098763\n",
      "Group 23, Epoch 3: G loss: 0.28449408837727136 vs. C loss: 0.1787047659357389\n",
      "Group 23, Epoch 4: G loss: 0.2502368884427207 vs. C loss: 0.10450853945480452\n",
      "Group 23, Epoch 5: G loss: 0.562196376493999 vs. C loss: 0.07799552670783468\n",
      "Group 23, Epoch 6: G loss: 0.9706699558666775 vs. C loss: 0.06726921929253472\n",
      "Group 23, Epoch 7: G loss: 0.9451539516448973 vs. C loss: 0.022967956468669903\n",
      "Group 23, Epoch 8: G loss: 0.983325491632734 vs. C loss: 0.0774132676823582\n",
      "Group 23, Epoch 9: G loss: 0.7618650555610657 vs. C loss: 0.22058506309986115\n",
      "Group 23, Epoch 10: G loss: 0.0706416666507721 vs. C loss: 0.20718346867296433\n",
      "Group 23, Epoch 11: G loss: 0.06161829712135451 vs. C loss: 0.2053298925360044\n",
      "Group 23, Epoch 12: G loss: 0.07398158950465065 vs. C loss: 0.17954416159126493\n",
      "Group 23, Epoch 13: G loss: 0.5739564980779376 vs. C loss: 0.05717453277773327\n",
      "Group 23, Epoch 14: G loss: 0.9269873687199184 vs. C loss: 0.010947113365141882\n",
      "Group 23, Epoch 15: G loss: 0.89245730979102 vs. C loss: 0.003785959470810162\n",
      "Group 23, Epoch 16: G loss: 0.9932107874325344 vs. C loss: 0.0005101318173627886\n",
      "Group 23, Epoch 17: G loss: 1.0294159684862412 vs. C loss: 0.00011602863701733037\n",
      "Group 23, Epoch 18: G loss: 1.0351394755499703 vs. C loss: 5.946033141097158e-05\n",
      "Group 23, Epoch 19: G loss: 1.0374427318572998 vs. C loss: 3.3522136517502884e-05\n",
      "Group 23, Epoch 20: G loss: 1.0273092099598478 vs. C loss: 5.1026813000337126e-05\n",
      "Group 24, Epoch 1: G loss: 0.2500708673681532 vs. C loss: 0.17753308018048605\n",
      "Group 24, Epoch 2: G loss: 0.2394078029053552 vs. C loss: 0.16315351757738325\n",
      "Group 24, Epoch 3: G loss: 0.42739256875855575 vs. C loss: 0.0791630388961898\n",
      "Group 24, Epoch 4: G loss: 0.5320873380771706 vs. C loss: 0.2085209223959181\n",
      "Group 24, Epoch 5: G loss: 0.02949853539466858 vs. C loss: 0.20729272895389136\n",
      "Group 24, Epoch 6: G loss: 0.023756194753306255 vs. C loss: 0.2075003381404612\n",
      "Group 25, Epoch 1: G loss: 0.28753924795559477 vs. C loss: 0.16223989178737006\n",
      "Group 25, Epoch 2: G loss: 0.2028964694057192 vs. C loss: 0.19145725584692422\n",
      "Group 25, Epoch 3: G loss: 0.07019041595714433 vs. C loss: 0.17753272586398652\n",
      "Group 25, Epoch 4: G loss: 0.3217672160693577 vs. C loss: 0.1203636849919955\n",
      "Group 25, Epoch 5: G loss: 0.6778727207865034 vs. C loss: 0.03936389792296621\n",
      "Group 25, Epoch 6: G loss: 0.3807954000575201 vs. C loss: 0.15523356033696067\n",
      "Group 26, Epoch 1: G loss: 0.3453276659761157 vs. C loss: 0.1785159177250332\n",
      "Group 26, Epoch 2: G loss: 0.2824261639799391 vs. C loss: 0.19696397334337234\n",
      "Group 26, Epoch 3: G loss: 0.15497546408857618 vs. C loss: 0.18656829165087804\n",
      "Group 26, Epoch 4: G loss: 0.18413606030600413 vs. C loss: 0.13550824506415263\n",
      "Group 26, Epoch 5: G loss: 0.5133818277290889 vs. C loss: 0.04428008219434155\n",
      "Group 26, Epoch 6: G loss: 0.6431638709136418 vs. C loss: 0.05238253198977974\n",
      "Group 27, Epoch 1: G loss: 0.29862763157912664 vs. C loss: 0.16364644964536032\n",
      "Group 27, Epoch 2: G loss: 0.23506995120218827 vs. C loss: 0.19646688799063364\n",
      "Group 27, Epoch 3: G loss: 0.10591943476881298 vs. C loss: 0.152270989285575\n",
      "Group 27, Epoch 4: G loss: 0.5159129364149911 vs. C loss: 0.07141625343097581\n",
      "Group 27, Epoch 5: G loss: 0.6743834870202201 vs. C loss: 0.023222112769467965\n",
      "Group 27, Epoch 6: G loss: 0.9450256024088178 vs. C loss: 0.006219464402723436\n",
      "Group 28, Epoch 1: G loss: 0.28051951825618743 vs. C loss: 0.20346037215656707\n",
      "Group 28, Epoch 2: G loss: 0.17682171378816877 vs. C loss: 0.18055979855772522\n",
      "Group 28, Epoch 3: G loss: 0.23008165529796057 vs. C loss: 0.1213603988289833\n",
      "Group 28, Epoch 4: G loss: 0.6466442687170846 vs. C loss: 0.049072131514549255\n",
      "Group 28, Epoch 5: G loss: 0.3519950228078024 vs. C loss: 0.20285530553923714\n",
      "Group 28, Epoch 6: G loss: 0.03881912476250104 vs. C loss: 0.1870774444606569\n",
      "Group 29, Epoch 1: G loss: 0.31519674190453123 vs. C loss: 0.17775136894649932\n",
      "Group 29, Epoch 2: G loss: 0.44813581194196433 vs. C loss: 0.174896412425571\n",
      "Group 29, Epoch 3: G loss: 0.1722523993679455 vs. C loss: 0.1555358668168386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 29, Epoch 4: G loss: 0.5202956838267191 vs. C loss: 0.07711753166384167\n",
      "Group 29, Epoch 5: G loss: 0.7379885707582746 vs. C loss: 0.016795886939184535\n",
      "Group 29, Epoch 6: G loss: 0.9283518007823398 vs. C loss: 0.005045171309676436\n",
      "Group 30, Epoch 1: G loss: 0.2826301519359861 vs. C loss: 0.1817514399687449\n",
      "Group 30, Epoch 2: G loss: 0.2183229156902858 vs. C loss: 0.18537824352582297\n",
      "Group 30, Epoch 3: G loss: 0.14053400648491723 vs. C loss: 0.14584070609675515\n",
      "Group 30, Epoch 4: G loss: 0.5247695241655622 vs. C loss: 0.07065880112349987\n",
      "Group 30, Epoch 5: G loss: 0.5838120460510253 vs. C loss: 0.06191426743235853\n",
      "Group 30, Epoch 6: G loss: 0.9697525961058481 vs. C loss: 0.02357319927412189\n",
      "Group 31, Epoch 1: G loss: 0.2990028832639967 vs. C loss: 0.18427561389075386\n",
      "Group 31, Epoch 2: G loss: 0.15163043056215558 vs. C loss: 0.1978724756174617\n",
      "Group 31, Epoch 3: G loss: 0.05867017784288951 vs. C loss: 0.19587275551425087\n",
      "Group 31, Epoch 4: G loss: 0.0747903792985848 vs. C loss: 0.17433436711629233\n",
      "Group 31, Epoch 5: G loss: 0.19397731167929513 vs. C loss: 0.17907899369796118\n",
      "Group 31, Epoch 6: G loss: 0.220704091659614 vs. C loss: 0.17343595127264658\n",
      "Group 32, Epoch 1: G loss: 0.2751151668173927 vs. C loss: 0.2119673622979058\n",
      "Group 32, Epoch 2: G loss: 0.2661137951271874 vs. C loss: 0.18228678239716425\n",
      "Group 32, Epoch 3: G loss: 0.23524416855403357 vs. C loss: 0.1511300876736641\n",
      "Group 32, Epoch 4: G loss: 0.31838529407978056 vs. C loss: 0.12424977289305793\n",
      "Group 32, Epoch 5: G loss: 0.35675622139658253 vs. C loss: 0.1388211250305176\n",
      "Group 32, Epoch 6: G loss: 0.2598957544990948 vs. C loss: 0.21265403429667154\n",
      "Group 33, Epoch 1: G loss: 0.2812964379787445 vs. C loss: 0.1841264632013109\n",
      "Group 33, Epoch 2: G loss: 0.17103707024029322 vs. C loss: 0.20161633690198263\n",
      "Group 33, Epoch 3: G loss: 0.04979900707091604 vs. C loss: 0.19591383801566228\n",
      "Group 33, Epoch 4: G loss: 0.08886797577142716 vs. C loss: 0.15014507373174033\n",
      "Group 33, Epoch 5: G loss: 0.5556044961724963 vs. C loss: 0.12024377327826287\n",
      "Group 33, Epoch 6: G loss: 0.5369868312563216 vs. C loss: 0.05966882883674569\n",
      "Group 34, Epoch 1: G loss: 0.28974906206130985 vs. C loss: 0.18991312550173864\n",
      "Group 34, Epoch 2: G loss: 0.20360511647803442 vs. C loss: 0.18992489245202804\n",
      "Group 34, Epoch 3: G loss: 0.1391190196786608 vs. C loss: 0.14095580577850345\n",
      "Group 34, Epoch 4: G loss: 0.2602275682347161 vs. C loss: 0.16427870591481528\n",
      "Group 34, Epoch 5: G loss: 0.6216556123324802 vs. C loss: 0.054854989258779414\n",
      "Group 34, Epoch 6: G loss: 0.808175710269383 vs. C loss: 0.022325515333149165\n",
      "Group 35, Epoch 1: G loss: 0.2867751176868167 vs. C loss: 0.17828804088963404\n",
      "Group 35, Epoch 2: G loss: 0.24774375898497444 vs. C loss: 0.18904504345522988\n",
      "Group 35, Epoch 3: G loss: 0.08976342848369054 vs. C loss: 0.15360617637634275\n",
      "Group 35, Epoch 4: G loss: 0.41885309815406796 vs. C loss: 0.07140249220861329\n",
      "Group 35, Epoch 5: G loss: 0.7903112973485673 vs. C loss: 0.01942805562996202\n",
      "Group 35, Epoch 6: G loss: 0.33906336676861554 vs. C loss: 0.205444292889701\n",
      "Group 36, Epoch 1: G loss: 0.2987789588315147 vs. C loss: 0.17854407595263588\n",
      "Group 36, Epoch 2: G loss: 0.23760463978563034 vs. C loss: 0.19390208025773367\n",
      "Group 36, Epoch 3: G loss: 0.09535347776753564 vs. C loss: 0.20152960220972696\n",
      "Group 36, Epoch 4: G loss: 0.06243732007486479 vs. C loss: 0.18521252564258048\n",
      "Group 36, Epoch 5: G loss: 0.16178007083279747 vs. C loss: 0.13350775092840197\n",
      "Group 36, Epoch 6: G loss: 0.5005138192858014 vs. C loss: 0.07032136329346232\n",
      "Group 37, Epoch 1: G loss: 0.2817933708429337 vs. C loss: 0.18320488764180076\n",
      "Group 37, Epoch 2: G loss: 0.2562657458441598 vs. C loss: 0.16930688752068412\n",
      "Group 37, Epoch 3: G loss: 0.30318087339401245 vs. C loss: 0.1718637529346678\n",
      "Group 37, Epoch 4: G loss: 0.5019985726901462 vs. C loss: 0.14643562667899662\n",
      "Group 37, Epoch 5: G loss: 0.5869220188685826 vs. C loss: 0.1854240405890677\n",
      "Group 37, Epoch 6: G loss: 0.5504660759653365 vs. C loss: 0.061446515222390495\n",
      "Group 38, Epoch 1: G loss: 0.3017443631376539 vs. C loss: 0.1726264887385898\n",
      "Group 38, Epoch 2: G loss: 0.19495447107723782 vs. C loss: 0.18004306571351159\n",
      "Group 38, Epoch 3: G loss: 0.27871979006699155 vs. C loss: 0.11526750731799336\n",
      "Group 38, Epoch 4: G loss: 0.21092210337519648 vs. C loss: 0.21539700279633203\n",
      "Group 38, Epoch 5: G loss: 0.0060355849830167636 vs. C loss: 0.2079151802592807\n",
      "Group 38, Epoch 6: G loss: 0.000389147869178227 vs. C loss: 0.20821136070622337\n",
      "Group 39, Epoch 1: G loss: 0.28012661933898925 vs. C loss: 0.1887615356180403\n",
      "Group 39, Epoch 2: G loss: 0.29661212095192496 vs. C loss: 0.1611557031671206\n",
      "Group 39, Epoch 3: G loss: 0.3312793578420367 vs. C loss: 0.1102924653225475\n",
      "Group 39, Epoch 4: G loss: 0.6547992740358625 vs. C loss: 0.032615248786492475\n",
      "Group 39, Epoch 5: G loss: 0.8726536750793458 vs. C loss: 0.01681512076821592\n",
      "Group 39, Epoch 6: G loss: 0.969983094079154 vs. C loss: 0.026156830830991063\n",
      "Group 40, Epoch 1: G loss: 0.2690379585538592 vs. C loss: 0.19397281938129005\n",
      "Group 40, Epoch 2: G loss: 0.2237364194222859 vs. C loss: 0.1985891577270296\n",
      "Group 40, Epoch 3: G loss: 0.07303222013371331 vs. C loss: 0.1999656988514794\n",
      "Group 40, Epoch 4: G loss: 0.06558037676981518 vs. C loss: 0.18713843656910792\n",
      "Group 40, Epoch 5: G loss: 0.157928019123418 vs. C loss: 0.15344089021285376\n",
      "Group 40, Epoch 6: G loss: 0.44566202163696295 vs. C loss: 0.10236695408821106\n",
      "Group 41, Epoch 1: G loss: 0.28543141186237336 vs. C loss: 0.18066944264703325\n",
      "Group 41, Epoch 2: G loss: 0.23214185088872913 vs. C loss: 0.19203704016076195\n",
      "Group 41, Epoch 3: G loss: 0.11835978861366 vs. C loss: 0.14607910397979948\n",
      "Group 41, Epoch 4: G loss: 0.5550104762826648 vs. C loss: 0.08438767285810576\n",
      "Group 41, Epoch 5: G loss: 0.5994603548731122 vs. C loss: 0.07791468314826487\n",
      "Group 41, Epoch 6: G loss: 0.9349815726280212 vs. C loss: 0.04018820635974407\n",
      "Group 42, Epoch 1: G loss: 0.2577729842492512 vs. C loss: 0.19077311538987687\n",
      "Group 42, Epoch 2: G loss: 0.15880070264850343 vs. C loss: 0.1889876955085331\n",
      "Group 42, Epoch 3: G loss: 0.09706740592207228 vs. C loss: 0.18914908087915847\n",
      "Group 42, Epoch 4: G loss: 0.12618555490459712 vs. C loss: 0.19680917428599465\n",
      "Group 42, Epoch 5: G loss: 0.20257328663553512 vs. C loss: 0.16400936908192107\n",
      "Group 42, Epoch 6: G loss: 0.2764689458268029 vs. C loss: 0.12885661837127474\n",
      "Group 43, Epoch 1: G loss: 0.2967801396335874 vs. C loss: 0.1889923761288325\n",
      "Group 43, Epoch 2: G loss: 0.24367984490735192 vs. C loss: 0.1935165011220508\n",
      "Group 43, Epoch 3: G loss: 0.16336475610733034 vs. C loss: 0.14390159977806938\n",
      "Group 43, Epoch 4: G loss: 0.6177991109234947 vs. C loss: 0.08512576607366402\n",
      "Group 43, Epoch 5: G loss: 0.19921881364924565 vs. C loss: 0.20434060071905455\n",
      "Group 43, Epoch 6: G loss: 0.023331712612083978 vs. C loss: 0.20522906631231308\n",
      "Group 44, Epoch 1: G loss: 0.29496816141264776 vs. C loss: 0.18479696578449675\n",
      "Group 44, Epoch 2: G loss: 0.3017348728009633 vs. C loss: 0.11867381632328032\n",
      "Group 44, Epoch 3: G loss: 0.6328758631433758 vs. C loss: 0.05517428213109574\n",
      "Group 44, Epoch 4: G loss: 0.7594987392425537 vs. C loss: 0.12723062601354387\n",
      "Group 44, Epoch 5: G loss: 0.4937560722231865 vs. C loss: 0.23560441533724466\n",
      "Group 44, Epoch 6: G loss: 0.13028044934783664 vs. C loss: 0.18496350281768373\n",
      "Group 45, Epoch 1: G loss: 0.3162154742649624 vs. C loss: 0.19295593599478403\n",
      "Group 45, Epoch 2: G loss: 0.26618249714374537 vs. C loss: 0.16972886357042527\n",
      "Group 45, Epoch 3: G loss: 0.27838765659502573 vs. C loss: 0.14888918772339824\n",
      "Group 45, Epoch 4: G loss: 0.6192817619868688 vs. C loss: 0.06229892931878567\n",
      "Group 45, Epoch 5: G loss: 0.30940987765789035 vs. C loss: 0.20028217633565268\n",
      "Group 45, Epoch 6: G loss: 0.01641044467687607 vs. C loss: 0.1994212062822448\n",
      "Group 46, Epoch 1: G loss: 0.2711578782115664 vs. C loss: 0.1738224915332264\n",
      "Group 46, Epoch 2: G loss: 0.1901804898466383 vs. C loss: 0.20469724469714698\n",
      "Group 46, Epoch 3: G loss: 0.04661846522774016 vs. C loss: 0.19646524513761202\n",
      "Group 46, Epoch 4: G loss: 0.0739591353705951 vs. C loss: 0.1657607737514708\n",
      "Group 46, Epoch 5: G loss: 0.32181581939969744 vs. C loss: 0.11542347073554993\n",
      "Group 46, Epoch 6: G loss: 0.505770297561373 vs. C loss: 0.06596617069509295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 47, Epoch 1: G loss: 0.2975806623697281 vs. C loss: 0.17134819593694472\n",
      "Group 47, Epoch 2: G loss: 0.23722956627607347 vs. C loss: 0.18203849014308715\n",
      "Group 47, Epoch 3: G loss: 0.1923136757952826 vs. C loss: 0.11783195783694585\n",
      "Group 47, Epoch 4: G loss: 0.6875008770397731 vs. C loss: 0.0570182278752327\n",
      "Group 47, Epoch 5: G loss: 0.39604947311537614 vs. C loss: 0.18960142963462404\n",
      "Group 47, Epoch 6: G loss: 0.1153222856777055 vs. C loss: 0.21771987775961557\n",
      "Group 48, Epoch 1: G loss: 0.27127315998077395 vs. C loss: 0.17564666436778176\n",
      "Group 48, Epoch 2: G loss: 0.2608324489423207 vs. C loss: 0.16417218579186335\n",
      "Group 48, Epoch 3: G loss: 0.3690724832671029 vs. C loss: 0.1042155416475402\n",
      "Group 48, Epoch 4: G loss: 0.8279061555862427 vs. C loss: 0.034124378528859876\n",
      "Group 48, Epoch 5: G loss: 0.6235282212495804 vs. C loss: 0.09297903709941441\n",
      "Group 48, Epoch 6: G loss: 0.9634098836353847 vs. C loss: 0.044129349705245756\n",
      "Group 49, Epoch 1: G loss: 0.27966216674872807 vs. C loss: 0.17870302498340607\n",
      "Group 49, Epoch 2: G loss: 0.21656168018068583 vs. C loss: 0.1723692003223631\n",
      "Group 49, Epoch 3: G loss: 0.17853594166891915 vs. C loss: 0.13471010368731287\n",
      "Group 49, Epoch 4: G loss: 0.7749034643173217 vs. C loss: 0.03269462468516496\n",
      "Group 49, Epoch 5: G loss: 0.9458380222320557 vs. C loss: 0.014139495815874802\n",
      "Group 49, Epoch 6: G loss: 0.8845508609499249 vs. C loss: 0.003830118764502307\n",
      "Group 50, Epoch 1: G loss: 0.2933098878179278 vs. C loss: 0.17342327700720891\n",
      "Group 50, Epoch 2: G loss: 0.22415316637073243 vs. C loss: 0.1936598900291655\n",
      "Group 50, Epoch 3: G loss: 0.19308531156608036 vs. C loss: 0.13984584477212694\n",
      "Group 50, Epoch 4: G loss: 0.5745026230812073 vs. C loss: 0.11681632366445331\n",
      "Group 50, Epoch 5: G loss: 0.35678532804761615 vs. C loss: 0.21084286438094244\n",
      "Group 50, Epoch 6: G loss: 0.08976463973522186 vs. C loss: 0.11581058883004719\n",
      "Group 51, Epoch 1: G loss: 0.29622952767780847 vs. C loss: 0.1874857627683216\n",
      "Group 51, Epoch 2: G loss: 0.22762271804468973 vs. C loss: 0.20815897319051954\n",
      "Group 51, Epoch 3: G loss: 0.08623927193028587 vs. C loss: 0.17857446604304847\n",
      "Group 51, Epoch 4: G loss: 0.14350271246262958 vs. C loss: 0.13379048142168257\n",
      "Group 51, Epoch 5: G loss: 0.5602437496185303 vs. C loss: 0.07957893692784838\n",
      "Group 51, Epoch 6: G loss: 0.5390650923763003 vs. C loss: 0.11850317484802671\n",
      "Group 52, Epoch 1: G loss: 0.3089645121778761 vs. C loss: 0.19463173217243615\n",
      "Group 52, Epoch 2: G loss: 0.38725115060806276 vs. C loss: 0.12913482387860617\n",
      "Group 52, Epoch 3: G loss: 0.4021510132721492 vs. C loss: 0.09872532801495658\n",
      "Group 52, Epoch 4: G loss: 0.6201033707175937 vs. C loss: 0.16476565351088843\n",
      "Group 52, Epoch 5: G loss: 0.23978791279452188 vs. C loss: 0.2138031290637122\n",
      "Group 52, Epoch 6: G loss: 0.10527795872517995 vs. C loss: 0.15655682442916763\n",
      "Group 53, Epoch 1: G loss: 0.2667906280074801 vs. C loss: 0.20985406471623314\n",
      "Group 53, Epoch 2: G loss: 0.27417962593691686 vs. C loss: 0.17817526393466523\n",
      "Group 53, Epoch 3: G loss: 0.30778064259460997 vs. C loss: 0.12809040976895228\n",
      "Group 53, Epoch 4: G loss: 0.623101646559579 vs. C loss: 0.053329342562291354\n",
      "Group 53, Epoch 5: G loss: 0.6396421892302377 vs. C loss: 0.10467619448900223\n",
      "Group 53, Epoch 6: G loss: 0.57515740777765 vs. C loss: 0.2176872309711244\n",
      "Group 54, Epoch 1: G loss: 0.28814053620610913 vs. C loss: 0.18243133690622118\n",
      "Group 54, Epoch 2: G loss: 0.269045204775674 vs. C loss: 0.15618296795421174\n",
      "Group 54, Epoch 3: G loss: 0.34275708113397874 vs. C loss: 0.08161776761213939\n",
      "Group 54, Epoch 4: G loss: 0.662534943648747 vs. C loss: 0.042185768795510135\n",
      "Group 54, Epoch 5: G loss: 0.6577615252562933 vs. C loss: 0.04534454995559321\n",
      "Group 54, Epoch 6: G loss: 0.99701281445367 vs. C loss: 0.035845982635186784\n",
      "Group 55, Epoch 1: G loss: 0.26116141804627013 vs. C loss: 0.18720470699999067\n",
      "Group 55, Epoch 2: G loss: 0.25852036263261524 vs. C loss: 0.20903241634368896\n",
      "Group 55, Epoch 3: G loss: 0.05988391595227377 vs. C loss: 0.1974609469374021\n",
      "Group 55, Epoch 4: G loss: 0.051901435319866455 vs. C loss: 0.1864985716011789\n",
      "Group 55, Epoch 5: G loss: 0.17758312565939766 vs. C loss: 0.18278159863419005\n",
      "Group 55, Epoch 6: G loss: 0.24653501127447403 vs. C loss: 0.17340598338180116\n",
      "Group 56, Epoch 1: G loss: 0.2939526992184775 vs. C loss: 0.19543356531196168\n",
      "Group 56, Epoch 2: G loss: 0.24439666994980405 vs. C loss: 0.18001538018385568\n",
      "Group 56, Epoch 3: G loss: 0.22568368507283076 vs. C loss: 0.17150159180164337\n",
      "Group 56, Epoch 4: G loss: 0.35917670726776124 vs. C loss: 0.10819139000442292\n",
      "Group 56, Epoch 5: G loss: 0.39075786513941635 vs. C loss: 0.13094096713595918\n",
      "Group 56, Epoch 6: G loss: 0.314303391958986 vs. C loss: 0.20711059868335724\n",
      "Group 57, Epoch 1: G loss: 0.2997006688799177 vs. C loss: 0.16916755669646796\n",
      "Group 57, Epoch 2: G loss: 0.36644696806158344 vs. C loss: 0.1822059717443254\n",
      "Group 57, Epoch 3: G loss: 0.13435537240334922 vs. C loss: 0.15766570303175184\n",
      "Group 57, Epoch 4: G loss: 0.5627343833446502 vs. C loss: 0.13338470541768602\n",
      "Group 57, Epoch 5: G loss: 0.5284084720270974 vs. C loss: 0.07431943693922625\n",
      "Group 57, Epoch 6: G loss: 0.6770806374294418 vs. C loss: 0.15500219249063069\n",
      "Group 58, Epoch 1: G loss: 0.30507354864052366 vs. C loss: 0.1854987872971429\n",
      "Group 58, Epoch 2: G loss: 0.25282333195209505 vs. C loss: 0.1744716101222568\n",
      "Group 58, Epoch 3: G loss: 0.14421946087053844 vs. C loss: 0.17467010021209717\n",
      "Group 58, Epoch 4: G loss: 0.3456515035458974 vs. C loss: 0.11873124374283685\n",
      "Group 58, Epoch 5: G loss: 0.630912298815591 vs. C loss: 0.04136393715937932\n",
      "Group 58, Epoch 6: G loss: 0.4484914965927601 vs. C loss: 0.20280610024929047\n",
      "Group 59, Epoch 1: G loss: 0.3027537388460977 vs. C loss: 0.16640612069103453\n",
      "Group 59, Epoch 2: G loss: 0.2539163116897855 vs. C loss: 0.1941395840711064\n",
      "Group 59, Epoch 3: G loss: 0.06919998111469405 vs. C loss: 0.1968486772643195\n",
      "Group 59, Epoch 4: G loss: 0.09962314388581685 vs. C loss: 0.15082903868622252\n",
      "Group 59, Epoch 5: G loss: 0.37686445755617953 vs. C loss: 0.18487149890926147\n",
      "Group 59, Epoch 6: G loss: 0.15177467465400696 vs. C loss: 0.15864058501190612\n",
      "Group 60, Epoch 1: G loss: 0.29162906578608927 vs. C loss: 0.18794735603862336\n",
      "Group 60, Epoch 2: G loss: 0.30520197323390413 vs. C loss: 0.1593611902660794\n",
      "Group 60, Epoch 3: G loss: 0.3809762294803347 vs. C loss: 0.13398352099789515\n",
      "Group 60, Epoch 4: G loss: 0.4919930534703391 vs. C loss: 0.09645613158742587\n",
      "Group 60, Epoch 5: G loss: 0.7740976797682899 vs. C loss: 0.11821788797775905\n",
      "Group 60, Epoch 6: G loss: 0.6623742359025139 vs. C loss: 0.13464993321233326\n",
      "Group 61, Epoch 1: G loss: 0.3235924614327294 vs. C loss: 0.18886609541045296\n",
      "Group 61, Epoch 2: G loss: 0.18132038350616184 vs. C loss: 0.20089907116360137\n",
      "Group 61, Epoch 3: G loss: 0.09046600205557687 vs. C loss: 0.18610905607541403\n",
      "Group 61, Epoch 4: G loss: 0.19344990934644427 vs. C loss: 0.15026278297106424\n",
      "Group 61, Epoch 5: G loss: 0.3889410602194922 vs. C loss: 0.13395741664701039\n",
      "Group 61, Epoch 6: G loss: 0.520502826997212 vs. C loss: 0.06971361053486665\n",
      "Group 62, Epoch 1: G loss: 0.2694991137300219 vs. C loss: 0.18804308440950182\n",
      "Group 62, Epoch 2: G loss: 0.17272717569555554 vs. C loss: 0.18365560885932708\n",
      "Group 62, Epoch 3: G loss: 0.16003209416355405 vs. C loss: 0.20664694243007234\n",
      "Group 62, Epoch 4: G loss: 0.10233363615615025 vs. C loss: 0.1894847055276235\n",
      "Group 62, Epoch 5: G loss: 0.13892542954002107 vs. C loss: 0.17773054871294233\n",
      "Group 62, Epoch 6: G loss: 0.3376885499273028 vs. C loss: 0.1394986112912496\n",
      "Group 63, Epoch 1: G loss: 0.2833159808601652 vs. C loss: 0.16583009560902914\n",
      "Group 63, Epoch 2: G loss: 0.24908277520111627 vs. C loss: 0.1825138678153356\n",
      "Group 63, Epoch 3: G loss: 0.18147565509591781 vs. C loss: 0.12895194192727408\n",
      "Group 63, Epoch 4: G loss: 0.5562042713165283 vs. C loss: 0.0531553506023354\n",
      "Group 63, Epoch 5: G loss: 0.5060352836336407 vs. C loss: 0.08642031086815728\n",
      "Group 63, Epoch 6: G loss: 0.9371581298964363 vs. C loss: 0.02813978976337239\n",
      "Group 64, Epoch 1: G loss: 0.28920557541506625 vs. C loss: 0.16915505131085715\n",
      "Group 64, Epoch 2: G loss: 0.3254205397197179 vs. C loss: 0.18310101992554131\n",
      "Group 64, Epoch 3: G loss: 0.12167996859976224 vs. C loss: 0.20581074059009552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 64, Epoch 4: G loss: 0.1034358331135341 vs. C loss: 0.17625594635804495\n",
      "Group 64, Epoch 5: G loss: 0.40220332315989904 vs. C loss: 0.059540184421671756\n",
      "Group 64, Epoch 6: G loss: 0.7565672397613524 vs. C loss: 0.03767858725041151\n",
      "Group 65, Epoch 1: G loss: 0.2796024365084512 vs. C loss: 0.1686610347694821\n",
      "Group 65, Epoch 2: G loss: 0.2639179383005415 vs. C loss: 0.18908357123533884\n",
      "Group 65, Epoch 3: G loss: 0.10244273883955819 vs. C loss: 0.19089035193125406\n",
      "Group 65, Epoch 4: G loss: 0.18424679381506784 vs. C loss: 0.12527122762468126\n",
      "Group 65, Epoch 5: G loss: 0.6918010813849313 vs. C loss: 0.06350808466474216\n",
      "Group 65, Epoch 6: G loss: 0.7683540148394449 vs. C loss: 0.060829070707162224\n",
      "Group 66, Epoch 1: G loss: 0.29308324115616935 vs. C loss: 0.1531900606221623\n",
      "Group 66, Epoch 2: G loss: 0.21067534429686413 vs. C loss: 0.19150494866900972\n",
      "Group 66, Epoch 3: G loss: 0.09381371693951743 vs. C loss: 0.14899731344646877\n",
      "Group 66, Epoch 4: G loss: 0.4685928796018873 vs. C loss: 0.1777158346441057\n",
      "Group 66, Epoch 5: G loss: 0.6143724373408725 vs. C loss: 0.040495825517508716\n",
      "Group 66, Epoch 6: G loss: 0.5968590898173195 vs. C loss: 0.09076392567820019\n",
      "Group 67, Epoch 1: G loss: 0.2886582889727184 vs. C loss: 0.1786723451481925\n",
      "Group 67, Epoch 2: G loss: 0.26488768288067405 vs. C loss: 0.18286419494284523\n",
      "Group 67, Epoch 3: G loss: 0.23105537976537432 vs. C loss: 0.11421821018060048\n",
      "Group 67, Epoch 4: G loss: 0.6870940029621124 vs. C loss: 0.08999845013022423\n",
      "Group 67, Epoch 5: G loss: 0.43914912843278486 vs. C loss: 0.23070681426260206\n",
      "Group 67, Epoch 6: G loss: 0.007659581636211701 vs. C loss: 0.20796925160619947\n",
      "Group 68, Epoch 1: G loss: 0.2844562807253429 vs. C loss: 0.16856326990657386\n",
      "Group 68, Epoch 2: G loss: 0.2317787647247314 vs. C loss: 0.16596350818872452\n",
      "Group 68, Epoch 3: G loss: 0.1629676559141704 vs. C loss: 0.1905998976694213\n",
      "Group 68, Epoch 4: G loss: 0.14188445402043207 vs. C loss: 0.1909030311637455\n",
      "Group 68, Epoch 5: G loss: 0.13863291357244764 vs. C loss: 0.1839326173067093\n",
      "Group 68, Epoch 6: G loss: 0.22799061323915207 vs. C loss: 0.2077001134554545\n",
      "Group 69, Epoch 1: G loss: 0.2784252162490572 vs. C loss: 0.17571675363514158\n",
      "Group 69, Epoch 2: G loss: 0.23337111047336037 vs. C loss: 0.17640007701185015\n",
      "Group 69, Epoch 3: G loss: 0.10695234026227678 vs. C loss: 0.1596777861316999\n",
      "Group 69, Epoch 4: G loss: 0.2996507814952305 vs. C loss: 0.17104455166392854\n",
      "Group 69, Epoch 5: G loss: 0.307122163261686 vs. C loss: 0.1417968546350797\n",
      "Group 69, Epoch 6: G loss: 0.5527892879077367 vs. C loss: 0.053652556820048235\n",
      "Group 70, Epoch 1: G loss: 0.2761586810861315 vs. C loss: 0.20272270838419595\n",
      "Group 70, Epoch 2: G loss: 0.23677110714571817 vs. C loss: 0.17013423144817352\n",
      "Group 70, Epoch 3: G loss: 0.2659938446113041 vs. C loss: 0.16018409695890215\n",
      "Group 70, Epoch 4: G loss: 0.185082578339747 vs. C loss: 0.2093703697125117\n",
      "Group 70, Epoch 5: G loss: 0.019461529329419136 vs. C loss: 0.2045164555311203\n",
      "Group 70, Epoch 6: G loss: 0.019412984432918685 vs. C loss: 0.2001580662197537\n",
      "Group 71, Epoch 1: G loss: 0.29575946671622144 vs. C loss: 0.16560172041257223\n",
      "Group 71, Epoch 2: G loss: 0.31133489438465667 vs. C loss: 0.17683042089144388\n",
      "Group 71, Epoch 3: G loss: 0.14294786197798595 vs. C loss: 0.16156118445926246\n",
      "Group 71, Epoch 4: G loss: 0.4810233661106654 vs. C loss: 0.09039735049009323\n",
      "Group 71, Epoch 5: G loss: 0.47971825344221936 vs. C loss: 0.09108258866601521\n",
      "Group 71, Epoch 6: G loss: 0.8917568206787109 vs. C loss: 0.011949574057426715\n",
      "Group 72, Epoch 1: G loss: 0.2839439826352255 vs. C loss: 0.1852769802014033\n",
      "Group 72, Epoch 2: G loss: 0.27575302507196153 vs. C loss: 0.1030430156323645\n",
      "Group 72, Epoch 3: G loss: 0.753889889376504 vs. C loss: 0.06720368957353963\n",
      "Group 72, Epoch 4: G loss: 0.9269358515739441 vs. C loss: 0.012415084677437939\n",
      "Group 72, Epoch 5: G loss: 0.7871266560895103 vs. C loss: 0.04185360033685962\n",
      "Group 72, Epoch 6: G loss: 1.0055642059871128 vs. C loss: 0.02317274724029832\n",
      "Group 73, Epoch 1: G loss: 0.23578154231820786 vs. C loss: 0.19266963998476663\n",
      "Group 73, Epoch 2: G loss: 0.21111369047846115 vs. C loss: 0.19623460620641708\n",
      "Group 73, Epoch 3: G loss: 0.06446569402303014 vs. C loss: 0.1981754791405466\n",
      "Group 73, Epoch 4: G loss: 0.07231285827500479 vs. C loss: 0.17419637160168755\n",
      "Group 73, Epoch 5: G loss: 0.26696883397442955 vs. C loss: 0.15991364419460297\n",
      "Group 73, Epoch 6: G loss: 0.20908604775156295 vs. C loss: 0.19622402969333863\n",
      "Group 73, Epoch 7: G loss: 0.06955989567296846 vs. C loss: 0.1880176862080892\n",
      "Group 74, Epoch 1: G loss: 0.2682589322328567 vs. C loss: 0.19015546474191877\n",
      "Group 74, Epoch 2: G loss: 0.23019869540418897 vs. C loss: 0.18929851717419097\n",
      "Group 74, Epoch 3: G loss: 0.2014774726969855 vs. C loss: 0.1594188784559568\n",
      "Group 74, Epoch 4: G loss: 0.493910551071167 vs. C loss: 0.1191100718246566\n",
      "Group 74, Epoch 5: G loss: 0.6277311095169613 vs. C loss: 0.0562048711710506\n",
      "Group 74, Epoch 6: G loss: 0.6764378294348716 vs. C loss: 0.17016931416259873\n",
      "Group 75, Epoch 1: G loss: 0.28339361718722755 vs. C loss: 0.16573026362392637\n",
      "Group 75, Epoch 2: G loss: 0.33140377828053064 vs. C loss: 0.18320778757333755\n",
      "Group 75, Epoch 3: G loss: 0.2120171019009182 vs. C loss: 0.1333808840976821\n",
      "Group 75, Epoch 4: G loss: 0.5911056603704179 vs. C loss: 0.08587464441855748\n",
      "Group 75, Epoch 5: G loss: 0.7856364556721278 vs. C loss: 0.014188989945169954\n",
      "Group 75, Epoch 6: G loss: 0.8397283818040576 vs. C loss: 0.0872851912346151\n",
      "Group 76, Epoch 1: G loss: 0.2749474103961672 vs. C loss: 0.17443063524034286\n",
      "Group 76, Epoch 2: G loss: 0.19748894338096892 vs. C loss: 0.2034055574072732\n",
      "Group 76, Epoch 3: G loss: 0.054624032548495696 vs. C loss: 0.19265448053677878\n",
      "Group 76, Epoch 4: G loss: 0.10593274895633968 vs. C loss: 0.16547264158725739\n",
      "Group 76, Epoch 5: G loss: 0.2540576606988907 vs. C loss: 0.14941730929745567\n",
      "Group 76, Epoch 6: G loss: 0.22236161636454718 vs. C loss: 0.2071636699967914\n",
      "Group 77, Epoch 1: G loss: 0.2741837003401348 vs. C loss: 0.1766685363319185\n",
      "Group 77, Epoch 2: G loss: 0.2273271019969668 vs. C loss: 0.18034040762318504\n",
      "Group 77, Epoch 3: G loss: 0.14105614594050817 vs. C loss: 0.14343895845943028\n",
      "Group 77, Epoch 4: G loss: 0.43764118807656427 vs. C loss: 0.12701421644952562\n",
      "Group 77, Epoch 5: G loss: 0.4086703628301621 vs. C loss: 0.14003526584969628\n",
      "Group 77, Epoch 6: G loss: 0.4857980212994985 vs. C loss: 0.13486356370978886\n",
      "Group 78, Epoch 1: G loss: 0.286478636094502 vs. C loss: 0.18063729090823066\n",
      "Group 78, Epoch 2: G loss: 0.19315516991274695 vs. C loss: 0.1422919515106413\n",
      "Group 78, Epoch 3: G loss: 0.45070863621575497 vs. C loss: 0.1284698752893342\n",
      "Group 78, Epoch 4: G loss: 0.39253879742962966 vs. C loss: 0.13345007557008\n",
      "Group 78, Epoch 5: G loss: 0.642797497340611 vs. C loss: 0.06519476945201556\n",
      "Group 78, Epoch 6: G loss: 0.9180231758526393 vs. C loss: 0.01919847529562604\n",
      "Group 79, Epoch 1: G loss: 0.2812991797924042 vs. C loss: 0.1807471720708741\n",
      "Group 79, Epoch 2: G loss: 0.22935176449162614 vs. C loss: 0.1736819346745809\n",
      "Group 79, Epoch 3: G loss: 0.16094489438193185 vs. C loss: 0.15657676756381989\n",
      "Group 79, Epoch 4: G loss: 0.40710469143731254 vs. C loss: 0.11910781264305115\n",
      "Group 79, Epoch 5: G loss: 0.585021334035056 vs. C loss: 0.053900946759515345\n",
      "Group 79, Epoch 6: G loss: 0.7476996609142849 vs. C loss: 0.07695602625608443\n",
      "Group 80, Epoch 1: G loss: 0.28225330795560566 vs. C loss: 0.18879631161689758\n",
      "Group 80, Epoch 2: G loss: 0.23691042831965858 vs. C loss: 0.17160437256097794\n",
      "Group 80, Epoch 3: G loss: 0.25851297080516816 vs. C loss: 0.11230708741479449\n",
      "Group 80, Epoch 4: G loss: 0.6692621162959507 vs. C loss: 0.039830709497133895\n",
      "Group 80, Epoch 5: G loss: 0.7701673396996089 vs. C loss: 0.07843660480446286\n",
      "Group 80, Epoch 6: G loss: 0.8192675871508462 vs. C loss: 0.12291079262892406\n",
      "Group 81, Epoch 1: G loss: 0.2751996559756143 vs. C loss: 0.1956382973326577\n",
      "Group 81, Epoch 2: G loss: 0.1855541474052838 vs. C loss: 0.19306509527895185\n",
      "Group 81, Epoch 3: G loss: 0.1467057515467916 vs. C loss: 0.16241945491896734\n",
      "Group 81, Epoch 4: G loss: 0.29325706490448544 vs. C loss: 0.10870779740313687\n",
      "Group 81, Epoch 5: G loss: 0.33598024440663193 vs. C loss: 0.14097818318340513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 81, Epoch 6: G loss: 0.3589323601020234 vs. C loss: 0.24245221416155496\n",
      "Group 82, Epoch 1: G loss: 0.27974961400032045 vs. C loss: 0.17216859095626405\n",
      "Group 82, Epoch 2: G loss: 0.23716042126928055 vs. C loss: 0.20447162290414175\n",
      "Group 82, Epoch 3: G loss: 0.07932440947209085 vs. C loss: 0.1909737955364916\n",
      "Group 82, Epoch 4: G loss: 0.10934337718146189 vs. C loss: 0.15685059130191803\n",
      "Group 82, Epoch 5: G loss: 0.4162283318383353 vs. C loss: 0.10758640203211044\n",
      "Group 82, Epoch 6: G loss: 0.5570766261645727 vs. C loss: 0.04630532405442662\n",
      "Group 83, Epoch 1: G loss: 0.29889396684510366 vs. C loss: 0.18967745701471964\n",
      "Group 83, Epoch 2: G loss: 0.40165925621986387 vs. C loss: 0.15886181344588599\n",
      "Group 83, Epoch 3: G loss: 0.2200446460928236 vs. C loss: 0.2070449458228217\n",
      "Group 83, Epoch 4: G loss: 0.06100853138736317 vs. C loss: 0.19305762234661314\n",
      "Group 83, Epoch 5: G loss: 0.13373596242495944 vs. C loss: 0.12087282207277085\n",
      "Group 83, Epoch 6: G loss: 0.7507831880024501 vs. C loss: 0.041675099482138954\n",
      "Group 83, Epoch 7: G loss: 0.5660022358809199 vs. C loss: 0.20497687657674155\n",
      "Group 83, Epoch 8: G loss: 0.041650603126202314 vs. C loss: 0.20713399185074702\n",
      "Group 84, Epoch 1: G loss: 0.29389170365674155 vs. C loss: 0.19428505334589216\n",
      "Group 84, Epoch 2: G loss: 0.2678581080266408 vs. C loss: 0.14673036833604178\n",
      "Group 84, Epoch 3: G loss: 0.29056166367871417 vs. C loss: 0.16485653817653656\n",
      "Group 84, Epoch 4: G loss: 0.31633401981421877 vs. C loss: 0.11639506535397637\n",
      "Group 84, Epoch 5: G loss: 0.6053758433886937 vs. C loss: 0.05466343586643537\n",
      "Group 84, Epoch 6: G loss: 0.7425780023847307 vs. C loss: 0.04743732372298837\n",
      "Group 85, Epoch 1: G loss: 0.26982782312801906 vs. C loss: 0.1799793408976661\n",
      "Group 85, Epoch 2: G loss: 0.19248800298997337 vs. C loss: 0.19720824973450768\n",
      "Group 85, Epoch 3: G loss: 0.07270456062895912 vs. C loss: 0.17511835942665735\n",
      "Group 85, Epoch 4: G loss: 0.2730581675257001 vs. C loss: 0.10336686174074809\n",
      "Group 85, Epoch 5: G loss: 0.7340619853564672 vs. C loss: 0.038365331995818354\n",
      "Group 85, Epoch 6: G loss: 0.6573038062879019 vs. C loss: 0.06634843804770045\n",
      "Group 86, Epoch 1: G loss: 0.28337247243949343 vs. C loss: 0.18951784405443403\n",
      "Group 86, Epoch 2: G loss: 0.18356374715055737 vs. C loss: 0.18697622583972082\n",
      "Group 86, Epoch 3: G loss: 0.1819993495941162 vs. C loss: 0.15371455914444393\n",
      "Group 86, Epoch 4: G loss: 0.3833783273186003 vs. C loss: 0.10729308509164387\n",
      "Group 86, Epoch 5: G loss: 0.7302019647189549 vs. C loss: 0.027734386631184157\n",
      "Group 86, Epoch 6: G loss: 0.3947194007890565 vs. C loss: 0.11739104323916964\n",
      "Group 87, Epoch 1: G loss: 0.3284484701497214 vs. C loss: 0.1751552571853002\n",
      "Group 87, Epoch 2: G loss: 0.4286775606019156 vs. C loss: 0.12559446030192906\n",
      "Group 87, Epoch 3: G loss: 0.4166393876075744 vs. C loss: 0.11315639730956818\n",
      "Group 87, Epoch 4: G loss: 0.505079043337277 vs. C loss: 0.11915735610657267\n",
      "Group 87, Epoch 5: G loss: 0.7788617478949683 vs. C loss: 0.12269466246167819\n",
      "Group 87, Epoch 6: G loss: 0.6919630799974713 vs. C loss: 0.037918236313594714\n",
      "Group 88, Epoch 1: G loss: 0.27259611657687594 vs. C loss: 0.16074757277965546\n",
      "Group 88, Epoch 2: G loss: 0.23010559997388294 vs. C loss: 0.1858908947971132\n",
      "Group 88, Epoch 3: G loss: 0.06761109105178288 vs. C loss: 0.16813030176692542\n",
      "Group 88, Epoch 4: G loss: 0.3267131422247206 vs. C loss: 0.1293323031730122\n",
      "Group 88, Epoch 5: G loss: 0.32620100762162885 vs. C loss: 0.1443598684337404\n",
      "Group 88, Epoch 6: G loss: 0.37404510421412335 vs. C loss: 0.12593696638941768\n",
      "Group 89, Epoch 1: G loss: 0.25910348296165464 vs. C loss: 0.19095283125837645\n",
      "Group 89, Epoch 2: G loss: 0.18110772924763816 vs. C loss: 0.18504707018534342\n",
      "Group 89, Epoch 3: G loss: 0.09769231804779599 vs. C loss: 0.18407427767912546\n",
      "Group 89, Epoch 4: G loss: 0.162480942266328 vs. C loss: 0.15812854882743624\n",
      "Group 89, Epoch 5: G loss: 0.2974186207566943 vs. C loss: 0.15052061610751682\n",
      "Group 89, Epoch 6: G loss: 0.43282991136823384 vs. C loss: 0.08097028980652492\n",
      "Group 90, Epoch 1: G loss: 0.26316294159208026 vs. C loss: 0.18970908059014213\n",
      "Group 90, Epoch 2: G loss: 0.14981794506311413 vs. C loss: 0.18852430085341135\n",
      "Group 90, Epoch 3: G loss: 0.10665485305445535 vs. C loss: 0.16705040882031122\n",
      "Group 90, Epoch 4: G loss: 0.337667794738497 vs. C loss: 0.11733826167053646\n",
      "Group 90, Epoch 5: G loss: 0.4424762325627463 vs. C loss: 0.079015228483412\n",
      "Group 90, Epoch 6: G loss: 0.8068042465618677 vs. C loss: 0.021792230972399313\n",
      "Group 91, Epoch 1: G loss: 0.2949879088572094 vs. C loss: 0.17110863824685416\n",
      "Group 91, Epoch 2: G loss: 0.284770387836865 vs. C loss: 0.17022116813394758\n",
      "Group 91, Epoch 3: G loss: 0.34975555581705914 vs. C loss: 0.16260385430521437\n",
      "Group 91, Epoch 4: G loss: 0.2008833463702883 vs. C loss: 0.2178097516298294\n",
      "Group 91, Epoch 5: G loss: 0.1386396171791213 vs. C loss: 0.13426142061750096\n",
      "Group 91, Epoch 6: G loss: 0.7450714298657009 vs. C loss: 0.031240030916200742\n",
      "Group 92, Epoch 1: G loss: 0.2973718447344643 vs. C loss: 0.19587473571300507\n",
      "Group 92, Epoch 2: G loss: 0.33690587197031296 vs. C loss: 0.09955722631679641\n",
      "Group 92, Epoch 3: G loss: 0.4623527735471725 vs. C loss: 0.16349555345045197\n",
      "Group 92, Epoch 4: G loss: 0.2315997501569135 vs. C loss: 0.23198407391707102\n",
      "Group 92, Epoch 5: G loss: 0.019500613478677615 vs. C loss: 0.2078454221288363\n",
      "Group 92, Epoch 6: G loss: 0.012116018017487865 vs. C loss: 0.20341709752877554\n",
      "Group 93, Epoch 1: G loss: 0.2900901172842298 vs. C loss: 0.15606074780225754\n",
      "Group 93, Epoch 2: G loss: 0.33601974930082046 vs. C loss: 0.1779973270992438\n",
      "Group 93, Epoch 3: G loss: 0.1550581855433328 vs. C loss: 0.1576105256875356\n",
      "Group 93, Epoch 4: G loss: 0.36617062389850613 vs. C loss: 0.1323807483745946\n",
      "Group 93, Epoch 5: G loss: 0.2891882171588285 vs. C loss: 0.2171095477210151\n",
      "Group 93, Epoch 6: G loss: 0.0056503327297312885 vs. C loss: 0.20688332451714408\n",
      "Group 94, Epoch 1: G loss: 0.28099732867309024 vs. C loss: 0.19856608907381693\n",
      "Group 94, Epoch 2: G loss: 0.2532135324818748 vs. C loss: 0.17283750987715188\n",
      "Group 94, Epoch 3: G loss: 0.2490056208201817 vs. C loss: 0.12081687814659543\n",
      "Group 94, Epoch 4: G loss: 0.4411808388573783 vs. C loss: 0.19225173857476976\n",
      "Group 94, Epoch 5: G loss: 0.1271821295576436 vs. C loss: 0.20436501751343408\n",
      "Group 94, Epoch 6: G loss: 0.05573685190507344 vs. C loss: 0.19154283073213366\n",
      "Group 95, Epoch 1: G loss: 0.2969770044088364 vs. C loss: 0.19284342394934761\n",
      "Group 95, Epoch 2: G loss: 0.3138055699212211 vs. C loss: 0.2059477170308431\n",
      "Group 95, Epoch 3: G loss: 0.15252115130424498 vs. C loss: 0.18679866360293496\n",
      "Group 95, Epoch 4: G loss: 0.19227326810359954 vs. C loss: 0.13841241515345043\n",
      "Group 95, Epoch 5: G loss: 0.6217546037265232 vs. C loss: 0.04129950598710113\n",
      "Group 95, Epoch 6: G loss: 0.47637078251157494 vs. C loss: 0.06568196995390786\n",
      "Group 96, Epoch 1: G loss: 0.30264449204717364 vs. C loss: 0.19128285513983834\n",
      "Group 96, Epoch 2: G loss: 0.3012646002428872 vs. C loss: 0.16753705259826449\n",
      "Group 96, Epoch 3: G loss: 0.22310226431914737 vs. C loss: 0.15869970122973123\n",
      "Group 96, Epoch 4: G loss: 0.49950306671006345 vs. C loss: 0.08977006127436955\n",
      "Group 96, Epoch 5: G loss: 0.23803290660892212 vs. C loss: 0.20932899994982615\n",
      "Group 96, Epoch 6: G loss: 0.029860958190900937 vs. C loss: 0.20692043834262427\n",
      "Group 97, Epoch 1: G loss: 0.254590351666723 vs. C loss: 0.19556391073597804\n",
      "Group 97, Epoch 2: G loss: 0.26438736745289393 vs. C loss: 0.16856326162815094\n",
      "Group 97, Epoch 3: G loss: 0.3794236711093358 vs. C loss: 0.08309147382775943\n",
      "Group 97, Epoch 4: G loss: 0.7906932728631156 vs. C loss: 0.06463278581698736\n",
      "Group 97, Epoch 5: G loss: 0.976148922102792 vs. C loss: 0.010369842975503868\n",
      "Group 97, Epoch 6: G loss: 0.7267075751508986 vs. C loss: 0.05347843659627769\n",
      "Group 97, Epoch 7: G loss: 1.053100541659764 vs. C loss: 0.014190506933826126\n",
      "Group 97, Epoch 8: G loss: 1.0369053465979439 vs. C loss: 0.004483310900266386\n",
      "Group 97, Epoch 9: G loss: 0.9433531284332275 vs. C loss: 0.006458280329954706\n",
      "Group 97, Epoch 10: G loss: 1.009163168498448 vs. C loss: 0.012130629908319356\n",
      "Group 97, Epoch 11: G loss: 1.0272338339260645 vs. C loss: 0.02464537291477124\n",
      "Group 97, Epoch 12: G loss: 0.3575654060712883 vs. C loss: 0.20636671284834543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 98, Epoch 1: G loss: 0.30067181161471773 vs. C loss: 0.1697730670372645\n",
      "Group 98, Epoch 2: G loss: 0.40319242988313947 vs. C loss: 0.148106517477168\n",
      "Group 98, Epoch 3: G loss: 0.37009972546781805 vs. C loss: 0.07950285863545205\n",
      "Group 98, Epoch 4: G loss: 0.8804970383644104 vs. C loss: 0.030434771492663354\n",
      "Group 98, Epoch 5: G loss: 0.8383060608591352 vs. C loss: 0.009933080472466018\n",
      "Group 98, Epoch 6: G loss: 0.8732955506869725 vs. C loss: 0.047831951537066035\n",
      "Group 99, Epoch 1: G loss: 0.3116260588169098 vs. C loss: 0.17815385593308344\n",
      "Group 99, Epoch 2: G loss: 0.3113731724875314 vs. C loss: 0.16045641733540428\n",
      "Group 99, Epoch 3: G loss: 0.34429826864174434 vs. C loss: 0.12333123468690448\n",
      "Group 99, Epoch 4: G loss: 0.825250266279493 vs. C loss: 0.06961858582993348\n",
      "Group 99, Epoch 5: G loss: 0.6709049071584429 vs. C loss: 0.0993183259334829\n",
      "Group 99, Epoch 6: G loss: 0.9685657211712427 vs. C loss: 0.007951766821659273\n",
      "Group 100, Epoch 1: G loss: 0.3504480183124542 vs. C loss: 0.15094224280781215\n",
      "Group 100, Epoch 2: G loss: 0.30997873076370785 vs. C loss: 0.17607738491561678\n",
      "Group 100, Epoch 3: G loss: 0.16405331811734608 vs. C loss: 0.1422402477926678\n",
      "Group 100, Epoch 4: G loss: 0.36219373515674047 vs. C loss: 0.18624610205491385\n",
      "Group 100, Epoch 5: G loss: 0.12334469301359995 vs. C loss: 0.208270443810357\n",
      "Group 100, Epoch 6: G loss: 0.1919227827872549 vs. C loss: 0.11284456236494912\n",
      "Group 101, Epoch 1: G loss: 0.25268426835536956 vs. C loss: 0.209635145134396\n",
      "Group 101, Epoch 2: G loss: 0.17790664008685522 vs. C loss: 0.19513577222824097\n",
      "Group 101, Epoch 3: G loss: 0.1157799550465175 vs. C loss: 0.16214635968208313\n",
      "Group 101, Epoch 4: G loss: 0.30053551111902516 vs. C loss: 0.13885092155800924\n",
      "Group 101, Epoch 5: G loss: 0.47814403431756153 vs. C loss: 0.16576812499099305\n",
      "Group 101, Epoch 6: G loss: 0.3291732490062714 vs. C loss: 0.1971106231212616\n",
      "Group 101, Epoch 7: G loss: 0.3799599298409054 vs. C loss: 0.1257562285496129\n",
      "Group 101, Epoch 8: G loss: 0.3722344745482717 vs. C loss: 0.20509671585427391\n",
      "Group 101, Epoch 9: G loss: 0.06391795266951833 vs. C loss: 0.20656682550907135\n",
      "Group 101, Epoch 10: G loss: 0.03819217735103198 vs. C loss: 0.20517499330970976\n"
     ]
    }
   ],
   "source": [
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer, add the start pos\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func='mse'):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.002) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.002) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "def mixed_model(generator, mapping_layer, discriminator, in_params):\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    inp = keras.layers.Input(shape=(in_params,))\n",
    "    start_pos = keras.layers.Input(shape = (2,))#tf.convert_to_tensor([0, 0], dtype=tf.float32)\n",
    "    rel = keras.layers.Input(shape = (7, note_group_size))#tf.zeros((5, note_group_size), dtype=tf.float32)\n",
    "    interm1 = generator(inp)\n",
    "    interm2 = mapping_layer(interm1)\n",
    "    end = discriminator(interm2)\n",
    "    model = keras.Model(inputs = inp, outputs = [interm1, interm2, end])\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.001) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "        \n",
    "    losses = [AlwaysZeroCustomLoss(), BoxCustomLoss(), GenerativeCustomLoss()];\n",
    "\n",
    "    model.compile(loss=losses,\n",
    "                  loss_weights=[1e-8, 1, 1],\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def conv_input(inp, extvar):\n",
    "#     Now it only uses single input\n",
    "    return inp;\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, GAN_PARAMS[\"g_input_size\"]));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# build models first, then train (it is faster in TF 2.0)\n",
    "\n",
    "def make_models():\n",
    "        \n",
    "    extvar[\"begin\"] = 0;\n",
    "    extvar[\"start_pos\"] = [256, 192];\n",
    "    extvar[\"length_multiplier\"] = 1;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    classifier_model = build_classifier_model();\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    \n",
    "    gmodel = generative_model(g_input_size, note_group_size * 4);\n",
    "    mapping_layer = KerasCustomMappingLayer(extvar);\n",
    "    mmodel = mixed_model(gmodel, mapping_layer, classifier_model, g_input_size);\n",
    "    \n",
    "    default_weights = mmodel.get_weights();\n",
    "    \n",
    "    return gmodel, mapping_layer, classifier_model, mmodel, default_weights;\n",
    "\n",
    "def set_extvar(models, extvar):\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    mapping_layer.set_extvar(extvar);\n",
    "    \n",
    "def reset_model_weights(models):\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    weights = default_weights;\n",
    "    mmodel.set_weights(weights);\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(models, begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    max_epoch = GAN_PARAMS[\"max_epoch\"];\n",
    "    good_epoch = GAN_PARAMS[\"good_epoch\"] - 1;\n",
    "    g_multiplier = GAN_PARAMS[\"g_epochs\"];\n",
    "    c_multiplier = GAN_PARAMS[\"c_epochs\"];\n",
    "    g_batch = GAN_PARAMS[\"g_batch\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    c_true_batch = GAN_PARAMS[\"c_true_batch\"];\n",
    "    c_false_batch = GAN_PARAMS[\"c_false_batch\"];\n",
    "    \n",
    "    reset_model_weights(models);\n",
    "    set_extvar(models, extvar);\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    \n",
    "    # see the summaries\n",
    "#     gmodel.summary()\n",
    "#     classifier_model.summary()\n",
    "#     mmodel.summary()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        gnoise = np.random.random((g_batch, g_input_size));\n",
    "        glabel = [np.zeros((g_batch, note_group_size * 4)), np.ones((g_batch,)), np.ones((g_batch,))]\n",
    "        ginput = conv_input(gnoise, extvar);\n",
    "        \n",
    "        # fit mmodel instead of gmodel\n",
    "        history = mmodel.fit(ginput, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[])\n",
    "        \n",
    "        pred_noise = np.random.random((c_false_batch, g_input_size));\n",
    "        pred_input = conv_input(pred_noise, extvar);\n",
    "        predicted_maps_data, predicted_maps_mapped, _predclass = mmodel.predict(pred_input);\n",
    "        new_false_maps = predicted_maps_mapped;\n",
    "        new_false_labels = np.zeros(c_false_batch);\n",
    "        \n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (c_true_batch,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "        \n",
    "    \n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[])\n",
    "        \n",
    "        # calculate the losses\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        \n",
    "        # delete the history to free memory\n",
    "        del history, history2\n",
    "        \n",
    "        # make a new set of notes\n",
    "        res_noise = np.random.random((1, g_input_size));\n",
    "        res_input = conv_input(res_noise, extvar);\n",
    "        _resgenerated, res_map, _resclass = mmodel.predict(res_input);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res_map, dtype=tf.float32));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        # good is (inside the map boundary)\n",
    "        if i >= good_epoch:\n",
    "#             current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            current_map = res_map;\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                # debugging options to check map integrity\n",
    "#                 print(tf.reduce_mean(current_map));\n",
    "#                 print(\"-----MAPLAYER-----\")\n",
    "#                 print(tf.reduce_mean(mapping_layer(conv_input(tf.convert_to_tensor(_resgenerated, dtype=\"float32\"), extvar))));\n",
    "#                 print(\"-----CMWS-----\")\n",
    "#                 print(tf.reduce_mean(construct_map_with_sliders(tf.convert_to_tensor(_resgenerated, dtype=\"float32\"), extvar=mapping_layer.extvar)));\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # from our testing, any random input generates nearly the same map\n",
    "            plot_noise = np.random.random((1, g_input_size));\n",
    "            plot_input = conv_input(plot_noise, extvar);\n",
    "            _plotgenerated, plot_mapped, _plotclass = mmodel.predict(plot_input);\n",
    "            plot_current_map(tf.convert_to_tensor(plot_mapped, dtype=tf.float32));\n",
    "    \n",
    "#     del mmodel, mapping_layer;\n",
    "    \n",
    "    return res_map.squeeze();\n",
    "#     onoise = np.random.random((1, g_input_size));\n",
    "    \n",
    "#     return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# generate the map (main function)\n",
    "# dist_multiplier in #6 is used here\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    models = make_models();\n",
    "    \n",
    "    print(\"# of groups: {}\".format(timestamps.shape[0] // note_group_size));\n",
    "    for i in range(timestamps.shape[0] // note_group_size):\n",
    "        z = generate_set(models, begin = i * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# generate a test map (debugging function)\n",
    "# dist_multiplier in #6 is used here\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    generate_set(begin = 3 * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success! finished on: 2022-05-03 18:51:44.070277\n"
     ]
    }
   ],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@2019/6/29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
